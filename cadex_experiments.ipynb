{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert D-FAUST dataset into .ply file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/usr/stud/srinivaa/code/CaDeX/resource/data/Humans/D-FAUST/50002_hips/pcl_seq/00000003.npz\"\n",
    "data = np.load(file_path)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(data['points'])\n",
    "o3d.io.write_point_cloud(\"data_3.ply\", pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the attributes of pcl_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points\n",
      "loc\n",
      "scale\n"
     ]
    }
   ],
   "source": [
    "for i in data.files:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the attributes of points_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points\n",
      "occupancies\n",
      "loc\n",
      "scale\n"
     ]
    }
   ],
   "source": [
    "points_seq = np.load(\"/usr/stud/srinivaa/code/CaDeX/resource/data/Humans/D-FAUST/50002_chicken_wings/points_seq/00000000.npz\")\n",
    "for i in points_seq.files:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, dirs, files = next(os.walk(\"/usr/stud/srinivaa/code/CaDeX/resource/data/Humans/D-FAUST\"))\n",
    "total_file_size = 0\n",
    "\n",
    "for i in range(len(dirs)): \n",
    "    if(dirs[i] != '50004_chicken_wings' and dirs[i] != '50020_hips' and dirs[i] != '50025_shake_arms' and dirs[i] != '50020_running_on_spot' and dirs[i] != '50007_light_hopping_loose'):\n",
    "        path_i,dirs_i,files_i = next(os.walk(os.path.join(path,dirs[i],'points_seq')))\n",
    "        total_file_size += len(files_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39920"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path, dirs, files = next(os.walk(\"/usr/stud/srinivaa/code/CaDeX/resource/data/Humans/D-FAUST\"))\n",
    "total_file_size = 0\n",
    "\n",
    "for i in range(len(dirs)): \n",
    "    if(dirs[i] != '50002_light_hopping_loose' and dirs[i] != '50004_punching' and dirs[i] != '50007_shake_shoulders' and dirs[i] != '50009_chicken_wings' and dirs[i] != '50020_chicken_wings' and dirs[i] != '50022_light_hopping_loose' and dirs[i] != '50025_light_hopping_loose' and dirs[i] != '50026_shake_arms' and dirs[i] != '50027_shake_shoulders' and dirs[i] != '50004_chicken_wings' and dirs[i] != '50020_hips' and dirs[i] != '50025_shake_arms' and dirs[i] != '50020_running_on_spot' and dirs[i] != '50007_light_hopping_loose'):\n",
    "        path_i,dirs_i,files_i = next(os.walk(os.path.join(path,dirs[i],'points_seq')))\n",
    "        total_file_size += len(files_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37867"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_file_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset of CaDeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset.oflow_dataset as oflow_dataset\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = os.path.join(dataset_folder, \"metadata.yaml\")\n",
    "\n",
    "        \n",
    "metadata = {c: {\"id\": c, \"name\": \"n/a\"} for c in categories}\n",
    "\n",
    "# Set index\n",
    "for c_idx, c in enumerate(categories):\n",
    "    metadata[c][\"idx\"] = c_idx #only one category: D-FAUST. contains single ID only\n",
    "\n",
    "# Get all models\n",
    "models = []\n",
    "split = \"train\"\n",
    "for c_idx, c in enumerate(categories):\n",
    "    subpath = os.path.join(dataset_folder, c) #subpath: /usr/stud/srinivaa/code/new_CaDeX/CaDeX/resource/data/Humans/D-FAUST\n",
    "    \n",
    "    if split is not None and os.path.exists(os.path.join(subpath, split + \".lst\")):\n",
    "        split_file = os.path.join(subpath, split + \".lst\") # for train mode: /usr/stud/srinivaa/code/new_CaDeX/CaDeX/resource/data/Humans/D-FAUST/train.lst\n",
    "        with open(split_file, \"r\") as f:\n",
    "            models_c = f.read().split(\"\\n\") # All files in train.lst for training mode\n",
    "    \n",
    "    models_c = list(filter(lambda x: len(x) > 0, models_c))\n",
    "    models_len = dataset.get_models_seq_len(subpath, models_c) # gives the total number .npz files in each model\n",
    "    models_c, start_idx = dataset.subdivide_into_sequences(models_c, models_len)\n",
    "    models += [\n",
    "        {\"category\": c, \"model\": m, \"start_idx\": start_idx[i]}\n",
    "        for i, m in enumerate(models_c)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/queues.py\", line 354, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "AttributeError: Can't get attribute 'init_dataset' on <module '__main__'>\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/queues.py\", line 354, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'init_dataset' on <module '__main__'>\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/queues.py\", line 354, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'init_dataset' on <module '__main__'>\n",
      "  File \"/usr/stud/srinivaa/anaconda3/envs/cadex/lib/python3.7/multiprocessing/queues.py\", line 354, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'init_dataset' on <module '__main__'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2359691/1450046044.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cadex/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cadex/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cadex/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cadex/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cadex/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def init_dataset(model_len):\n",
    "\n",
    "    samples = []\n",
    "    for idx in tqdm(range(model_len)):\n",
    "        category = models[idx][\"category\"]\n",
    "        model = models[idx][\"model\"]\n",
    "        start_idx = models[idx][\"start_idx\"]\n",
    "        c_idx = metadata[category][\"idx\"]\n",
    "\n",
    "        model_path = os.path.join(dataset_folder, category, model)\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        for field_name, field in fields.items():\n",
    "            field_data = field.load(model_path, idx, c_idx, start_idx)\n",
    "                \n",
    "\n",
    "            if isinstance(field_data, dict):\n",
    "                for k, v in field_data.items():\n",
    "                    if k is None:\n",
    "                        data[field_name] = v\n",
    "                    else:\n",
    "                        data[\"%s.%s\" % (field_name, k)] = v\n",
    "            else:\n",
    "                data[field_name] = field_data\n",
    "        \n",
    "        samples.append(data)\n",
    "\n",
    "results = pool.map(init_dataset, [row for row in models])\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumansDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_folder,\n",
    "        fields,\n",
    "        split=None,\n",
    "        categories=None,\n",
    "        no_except=True,\n",
    "        transform=None,\n",
    "        length_sequence=17,\n",
    "        n_files_per_sequence=-1,\n",
    "        offset_sequence=0,\n",
    "        ex_folder_name=\"pcl_seq\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Attributes\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.fields = fields\n",
    "        self.no_except = no_except\n",
    "        self.transform = transform\n",
    "        self.length_sequence = length_sequence\n",
    "        self.n_files_per_sequence = n_files_per_sequence\n",
    "        self.offset_sequence = offset_sequence\n",
    "        self.ex_folder_name = ex_folder_name\n",
    "       \n",
    "        # Read metadata file\n",
    "        metadata_file = os.path.join(dataset_folder, \"metadata.yaml\")\n",
    "\n",
    "        \n",
    "        self.metadata = {c: {\"id\": c, \"name\": \"n/a\"} for c in categories}\n",
    "\n",
    "        # Set index\n",
    "        for c_idx, c in enumerate(categories):\n",
    "            self.metadata[c][\"idx\"] = c_idx #only one category: D-FAUST. contains single ID only\n",
    "\n",
    "        # Get all models\n",
    "        self.models = []\n",
    "        for c_idx, c in enumerate(categories):\n",
    "            subpath = os.path.join(dataset_folder, c) #subpath: /usr/stud/srinivaa/code/new_CaDeX/CaDeX/resource/data/Humans/D-FAUST\n",
    "           \n",
    "            if split is not None and os.path.exists(os.path.join(subpath, split + \".lst\")):\n",
    "                split_file = os.path.join(subpath, split + \".lst\") # for train mode: /usr/stud/srinivaa/code/new_CaDeX/CaDeX/resource/data/Humans/D-FAUST/train.lst\n",
    "                with open(split_file, \"r\") as f:\n",
    "                    models_c = f.read().split(\"\\n\") # All files in train.lst for training mode\n",
    "           \n",
    "            models_c = list(filter(lambda x: len(x) > 0, models_c))\n",
    "            models_len = self.get_models_seq_len(subpath, models_c) # gives the total number .npz files in each model\n",
    "            models_c, start_idx = self.subdivide_into_sequences(models_c, models_len)\n",
    "            self.models += [\n",
    "                {\"category\": c, \"model\": m, \"start_idx\": start_idx[i]}\n",
    "                for i, m in enumerate(models_c)\n",
    "            ]\n",
    "        \n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "       return len(self.models)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        category = self.models[idx][\"category\"]\n",
    "        model = self.models[idx][\"model\"]\n",
    "        start_idx = self.models[idx][\"start_idx\"]\n",
    "        c_idx = self.metadata[category][\"idx\"]\n",
    "\n",
    "        model_path = os.path.join(self.dataset_folder, category, model)\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        \n",
    "        for field_name, field in self.fields.items():\n",
    "            field_data = field.load(model_path, idx, c_idx, start_idx)\n",
    "                \n",
    "\n",
    "            if isinstance(field_data, dict):\n",
    "                for k, v in field_data.items():\n",
    "                    if k is None:\n",
    "                        data[field_name] = v\n",
    "                    else:\n",
    "                        data[\"%s.%s\" % (field_name, k)] = v\n",
    "            else:\n",
    "                data[field_name] = field_data\n",
    "           \n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get_models_seq_len(self, subpath, models):\n",
    "        \"\"\"Returns the sequence length of a specific model.\n",
    "\n",
    "        This is a little \"hacky\" as we assume the existence of the folder\n",
    "        self.ex_folder_name. However, in our case this is always given.\n",
    "\n",
    "        Args:\n",
    "            subpath (str): subpath of model category\n",
    "            models (list): list of model names\n",
    "        \"\"\"\n",
    "        ex_folder_name = self.ex_folder_name\n",
    "        models_seq_len = []\n",
    "        for m in models:\n",
    "            _sublist = [\n",
    "                f for f in os.listdir(os.path.join(subpath, m, ex_folder_name)) if \"_\" not in f\n",
    "            ]\n",
    "            models_seq_len.append(len(_sublist))\n",
    "        # models_seq_len = [len(os.listdir(os.path.join(subpath, m, ex_folder_name))) for m in models]\n",
    "        return models_seq_len\n",
    "\n",
    "    def subdivide_into_sequences(self, models, models_len):\n",
    "        \"\"\"Subdivides model sequence into smaller sequences.\n",
    "\n",
    "        Args:\n",
    "            models (list): list of model names\n",
    "            models_len (list): list of lengths of model sequences\n",
    "        \"\"\"\n",
    "        length_sequence = self.length_sequence\n",
    "        n_files_per_sequence = self.n_files_per_sequence\n",
    "        offset_sequence = self.offset_sequence\n",
    "\n",
    "        # Remove files before offset\n",
    "        models_len = [l - offset_sequence for l in models_len]\n",
    "\n",
    "        # Reduce to maximum number of files that should be considered\n",
    "        if n_files_per_sequence > 0:\n",
    "            models_len = [min(n_files_per_sequence, l) for l in models_len]\n",
    "\n",
    "        models_out = []\n",
    "        start_idx = []\n",
    "        for idx, model in enumerate(models):\n",
    "            for n in range(0, models_len[idx] - length_sequence + 1):\n",
    "                models_out.append(model)\n",
    "                start_idx.append(n + offset_sequence)\n",
    "\n",
    "        return models_out, start_idx   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    \"\"\"Returns transform objects.\n",
    "\n",
    "    Args:\n",
    "        cfg (yaml config): yaml config object\n",
    "    \"\"\"\n",
    "    n_pcl = 100\n",
    "    n_pt = 512\n",
    "    n_pt_eval = 10000\n",
    "\n",
    "    transf_pt = oflow_dataset.SubsamplePoints(n_pt)\n",
    "    transf_pt_val = oflow_dataset.SubsamplePointsSeq(n_pt_eval, random=False)\n",
    "    transf_pcl_val = oflow_dataset.SubsamplePointcloudSeq(n_pt_eval, random=False)\n",
    "    transf_pcl = oflow_dataset.SubsamplePointcloudSeq(n_pcl, connected_samples=True)\n",
    "\n",
    "    return transf_pt, transf_pt_val, transf_pcl, transf_pcl_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_fields(mode):\n",
    "    \"\"\"Returns data fields.\n",
    "\n",
    "    Args:\n",
    "        mode (str): mode (train|val|test)\n",
    "        cfg (yaml config): yaml config object\n",
    "    \"\"\"\n",
    "    fields = {}\n",
    "    seq_len_train = 17\n",
    "   \n",
    "    seq_len_val = seq_len_train\n",
    "    p_folder = \"points_seq\" # points_seq: contains information regarding the points and their corresponding occupancy values\n",
    "    pcl_folder = \"pcl_seq\" #pcl_seq : contains information regarding the points, scale and loc\n",
    "    mesh_folder = \"mesh_seq\" #mesh_seq: non-existent. Utilize this to have a file containing points and faces for each model.\n",
    "    generate_interpolate = False #False\n",
    "    unpackbits = True # True\n",
    "    \n",
    "    training_all = False\n",
    "    \n",
    "    n_training_frames = 8\n",
    "\n",
    "    # Transformation\n",
    "    transf_pt, transf_pt_val, transf_pcl, transf_pcl_val = get_transforms()\n",
    "\n",
    "    # Fields\n",
    "    pts_iou_field = oflow_dataset.PointsSubseqField\n",
    "    pts_corr_field = oflow_dataset.PointCloudSubseqField\n",
    "\n",
    "    # MeshSubseqField can be used to load mesh fields\n",
    "\n",
    "  \n",
    "    not_choose_last = False\n",
    "    training_multi_files = False\n",
    "    \n",
    "    loss_recon = \"true\"\n",
    "    loss_corr = \"true\"\n",
    "\n",
    "    if mode == \"train\":\n",
    "        if loss_recon:\n",
    "            if training_all:\n",
    "                fields[\"points\"] = pts_iou_field(\n",
    "                    p_folder,\n",
    "                    transform=transf_pt,\n",
    "                    all_steps=True,\n",
    "                    seq_len=seq_len_train,\n",
    "                    unpackbits=unpackbits,\n",
    "                    use_multi_files=training_multi_files,\n",
    "                )\n",
    "            else:\n",
    "                fields[\"points\"] = pts_iou_field(\n",
    "                    p_folder,\n",
    "                    sample_nframes=n_training_frames,\n",
    "                    transform=transf_pt,\n",
    "                    seq_len=seq_len_train,\n",
    "                    fixed_time_step=0,\n",
    "                    unpackbits=unpackbits,\n",
    "                    use_multi_files=training_multi_files,\n",
    "                )\n",
    "            fields[\"points_t\"] = pts_iou_field(\n",
    "                p_folder,\n",
    "                transform=transf_pt,\n",
    "                seq_len=seq_len_train,\n",
    "                unpackbits=unpackbits,\n",
    "                not_choose_last=not_choose_last,\n",
    "                use_multi_files=training_multi_files,\n",
    "            )\n",
    "\n",
    "\n",
    "            # fields[\"mesh\"] = oflow_dataset.MeshField(\n",
    "            # mesh_folder, seq_len=seq_len_val)\n",
    "\n",
    "    # only training can be boost by multi-files\n",
    "    # modify here, if not train, val should also load the same as the test\n",
    "    else:\n",
    "        fields[\"points\"] = pts_iou_field(\n",
    "            p_folder,\n",
    "            transform=transf_pt_val,\n",
    "            all_steps=True,\n",
    "            seq_len=seq_len_val,\n",
    "            unpackbits=unpackbits,\n",
    "        )\n",
    "        fields[\n",
    "            \"points_mesh\"\n",
    "        ] = pts_corr_field(  # ? this if for correspondence? Checked, this is for chamfer distance, make sure that because here we use tranforms, teh pts in config file must be 100000\n",
    "            pcl_folder, transform=transf_pcl_val, seq_len=seq_len_val\n",
    "        )\n",
    "    # Connectivity Loss:\n",
    "    if loss_corr:\n",
    "        fields[\"pointcloud\"] = pts_corr_field(\n",
    "            pcl_folder,\n",
    "            transform=transf_pcl,\n",
    "            seq_len=seq_len_train,\n",
    "            use_multi_files=training_multi_files,\n",
    "        )\n",
    "        # fields[\"pointcloud\"] = oflow_dataset.MeshField(\n",
    "        #     mesh_folder, seq_len=seq_len_val)\n",
    "    if mode == \"test\" and generate_interpolate:\n",
    "        fields[\"mesh\"] = oflow_dataset.MeshSubseqField(\n",
    "            mesh_folder, seq_len=seq_len_val, only_end_points=True\n",
    "        )\n",
    "    fields[\"oflow_idx\"] = oflow_dataset.IndexField()\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = get_data_fields(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"/usr/data/cvpr_shared/marvin/Data/CaDeX/data/Humans\"\n",
    "categories = [\"D-FAUST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = oflow_dataset.HumansDataset(\n",
    "        dataset_folder,\n",
    "        fields,\n",
    "        split=\"train\",\n",
    "        categories=categories,\n",
    "        length_sequence=17,\n",
    "        n_files_per_sequence=-1,\n",
    "        offset_sequence=15,\n",
    "        ex_folder_name=\"mesh_seq\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'arrray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1545676/425756422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mesh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/usr/data/cvpr_shared/marvin/Data/CaDeX/data/Humans/D-FAUST/50002_chicken_wings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/new_CaDeX/CaDeX/dataset/oflow_dataset/fields.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, model_path, idx, c_idx, start_idx, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m         data = {\n\u001b[1;32m    715\u001b[0m             \u001b[0;34m\"vertices\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmesh_vertices_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0;34m\"triangles\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmesh_face_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;34m\"time\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_time_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         }\n",
      "\u001b[0;32m~/anaconda3/envs/cadex/lib/python3.7/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0;32m--> 314\u001b[0;31m                                  \"{!r}\".format(__name__, attr))\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'arrray'"
     ]
    }
   ],
   "source": [
    "dataset.fields['mesh'].load(\"/usr/data/cvpr_shared/marvin/Data/CaDeX/data/Humans/D-FAUST/50002_chicken_wings\",0,0,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HumansDataset(\n",
    "        dataset_folder,\n",
    "        fields,\n",
    "        split=\"train\",\n",
    "        categories=categories,\n",
    "        length_sequence=17,\n",
    "        n_files_per_sequence=-1,\n",
    "        offset_sequence=15,\n",
    "        ex_folder_name=\"mesh_seq_downsampled\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'points': array([[[ 0.32808763, -0.22058581, -0.03803909],\n",
       "         [-0.31480175, -0.20762537,  0.4784967 ],\n",
       "         [-0.06073018,  0.38500583, -0.52290946],\n",
       "         ...,\n",
       "         [-0.19945647,  0.21658814,  0.3572511 ],\n",
       "         [ 0.10739658,  0.38998714, -0.32283428],\n",
       "         [-0.5370004 , -0.5385551 , -0.2202155 ]],\n",
       " \n",
       "        [[ 0.52358097, -0.3189546 ,  0.40607086],\n",
       "         [ 0.38362423,  0.2854688 ,  0.42075822],\n",
       "         [ 0.09471595,  0.53994507,  0.5305025 ],\n",
       "         ...,\n",
       "         [-0.15432149,  0.2493386 ,  0.31248918],\n",
       "         [-0.25835425,  0.50031364, -0.1383201 ],\n",
       "         [-0.4523126 , -0.38836432, -0.00709423]],\n",
       " \n",
       "        [[ 0.25139698,  0.4594816 ,  0.5077217 ],\n",
       "         [ 0.26197058,  0.52350456,  0.29217276],\n",
       "         [ 0.48784742,  0.23342201,  0.3194862 ],\n",
       "         ...,\n",
       "         [ 0.26933873, -0.4507254 ,  0.07091916],\n",
       "         [ 0.40251952, -0.22128266,  0.21579437],\n",
       "         [-0.34766042,  0.22213608,  0.51825607]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.4715916 , -0.2578062 , -0.35644892],\n",
       "         [-0.11217718, -0.17818238, -0.34357077],\n",
       "         [-0.29824585, -0.09075361,  0.00703444],\n",
       "         ...,\n",
       "         [-0.4868585 , -0.5078471 ,  0.34896123],\n",
       "         [-0.3203763 , -0.14231613, -0.4698255 ],\n",
       "         [-0.4395129 ,  0.42791134,  0.15493748]],\n",
       " \n",
       "        [[ 0.52383006, -0.14177111, -0.46280503],\n",
       "         [ 0.4971927 ,  0.341681  , -0.48252216],\n",
       "         [-0.10747711, -0.1551892 ,  0.20646153],\n",
       "         ...,\n",
       "         [-0.06864124,  0.09622762,  0.24561687],\n",
       "         [ 0.44866797,  0.05599593, -0.5107458 ],\n",
       "         [-0.3672156 ,  0.4962757 ,  0.1883787 ]],\n",
       " \n",
       "        [[ 0.21474884, -0.28342518,  0.06279937],\n",
       "         [ 0.03729519,  0.5490914 ,  0.30576172],\n",
       "         [ 0.37132844, -0.23775358, -0.51877934],\n",
       "         ...,\n",
       "         [-0.3259036 , -0.44822952, -0.00513011],\n",
       "         [ 0.15192714, -0.10794125,  0.4122459 ],\n",
       "         [-0.10871325, -0.2935277 , -0.33794844]]], dtype=float32),\n",
       " 'points.occ': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'points.time': array([0.    , 0.0625, 0.1875, 0.25  , 0.3125, 0.5625, 0.6875, 0.9375],\n",
       "       dtype=float32),\n",
       " 'points_t': array([[-0.14698486, -0.05591848,  0.47598988],\n",
       "        [-0.4613128 ,  0.24708728,  0.01190328],\n",
       "        [-0.11307955, -0.422206  , -0.07518563],\n",
       "        ...,\n",
       "        [ 0.335768  , -0.4479104 ,  0.2584818 ],\n",
       "        [-0.06711765,  0.32469004, -0.3186429 ],\n",
       "        [-0.0306419 ,  0.24463925,  0.50732476]], dtype=float32),\n",
       " 'points_t.occ': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " 'points_t.time': array(0.875, dtype=float32),\n",
       " 'pointcloud': array([[[ 5.26428223e-02, -1.35131836e-01,  2.34527569e-02],\n",
       "         [-1.17034903e-02,  3.00537139e-01,  3.67736816e-02],\n",
       "         [ 2.87841797e-01,  3.33251990e-02,  1.08871469e-02],\n",
       "         ...,\n",
       "         [-1.17309570e-01, -3.49853486e-01,  1.23901367e-02],\n",
       "         [ 3.64781241e-04, -1.12915039e-01, -9.93652344e-02],\n",
       "         [-2.18872070e-01,  1.93725586e-01, -4.87060547e-02]],\n",
       " \n",
       "        [[ 5.35206422e-02, -1.34823963e-01,  2.25628894e-02],\n",
       "         [-9.73170996e-03,  3.00650030e-01,  3.66901644e-02],\n",
       "         [ 2.90297419e-01,  3.25674526e-02,  1.12885339e-02],\n",
       "         ...,\n",
       "         [-1.16189718e-01, -3.38647276e-01,  1.77114084e-02],\n",
       "         [ 3.50752001e-04, -1.12488836e-01, -1.00280307e-01],\n",
       "         [-2.17185989e-01,  1.94588646e-01, -4.92634624e-02]],\n",
       " \n",
       "        [[ 5.45997098e-02, -1.34603277e-01,  2.12015044e-02],\n",
       "         [-7.89807085e-03,  3.00790220e-01,  3.64253223e-02],\n",
       "         [ 2.92625427e-01,  3.26374099e-02,  1.17963133e-02],\n",
       "         ...,\n",
       "         [-1.15174472e-01, -3.27270389e-01,  2.32084543e-02],\n",
       "         [ 3.43397638e-04, -1.11832418e-01, -1.01069778e-01],\n",
       "         [-2.15170890e-01,  1.95787892e-01, -4.92096022e-02]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 5.55417649e-02, -1.36715561e-01,  1.99746508e-02],\n",
       "         [ 1.20588588e-02,  3.02793682e-01,  3.51967253e-02],\n",
       "         [ 3.11177820e-01,  3.19546722e-02,  1.43041266e-02],\n",
       "         ...,\n",
       "         [-1.07530609e-01, -2.53200203e-01,  6.25895783e-02],\n",
       "         [-7.19887845e-04, -1.14262909e-01, -1.00482784e-01],\n",
       "         [-2.00767249e-01,  2.06865147e-01, -5.26408851e-02]],\n",
       " \n",
       "        [[ 5.53460345e-02, -1.37181222e-01,  2.01516394e-02],\n",
       "         [ 1.31738978e-02,  3.02411228e-01,  3.52763906e-02],\n",
       "         [ 3.12386543e-01,  3.16564031e-02,  1.38716400e-02],\n",
       "         ...,\n",
       "         [-1.07391231e-01, -2.51910686e-01,  6.39205351e-02],\n",
       "         [ 7.10407970e-04, -1.14920273e-01, -1.00070439e-01],\n",
       "         [-2.00471327e-01,  2.06884876e-01, -5.25518917e-02]],\n",
       " \n",
       "        [[ 5.46890870e-02, -1.37428090e-01,  2.14833450e-02],\n",
       "         [ 1.42744444e-02,  3.01920325e-01,  3.51900533e-02],\n",
       "         [ 3.13798279e-01,  3.12099215e-02,  1.34702483e-02],\n",
       "         ...,\n",
       "         [-1.07391723e-01, -2.51269847e-01,  6.47280738e-02],\n",
       "         [ 1.44422567e-03, -1.15723573e-01, -9.96148959e-02],\n",
       "         [-1.99895784e-01,  2.06787288e-01, -5.21095023e-02]]],\n",
       "       dtype=float32),\n",
       " 'pointcloud.time': array([0.    , 0.0625, 0.125 , 0.1875, 0.25  , 0.3125, 0.375 , 0.4375,\n",
       "        0.5   , 0.5625, 0.625 , 0.6875, 0.75  , 0.8125, 0.875 , 0.9375,\n",
       "        1.    ], dtype=float32),\n",
       " 'oflow_idx': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b285100ef572e8bce26941eab5cbe7b2c48dcb7c648cf35e85b0f6ae5c8f083a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
