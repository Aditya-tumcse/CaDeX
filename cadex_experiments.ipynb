{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# import h5py\n",
    "# import open3d as o3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert D-FAUST dataset into .ply file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/usr/stud/srinivaa/storage/slurm/cadex/dfaust_resgistered/Humans/50004_hips/mesh_seq_registered/00000000.npz\"\n",
    "data = np.load(file_path)\n",
    "# pcd = o3d.geometry.PointCloud()\n",
    "# pcd.points = o3d.utility.Vector3dVector(data['points'])\n",
    "# o3d.io.write_point_cloud(\"registered_dfaust.ply\", pcd)\n",
    "\n",
    "o3d_mesh = o3d.geometry.TriangleMesh()\n",
    "o3d_mesh.vertices = o3d.utility.Vector3dVector(data[\"points\"]) # verify what is the name of the attribute for vertices in data dictionary. It should be vertices as given in load method in MeshField class.\n",
    "o3d_mesh.triangles = o3d.utility.Vector3iVector(data[\"triangles\"])\n",
    "o3d.io.write_triangle_mesh(\"registered_dfaust.ply\",o3d_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from registred DFAUST scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/usr/stud/srinivaa/storage/slurm/cadex/dfaust_resgistered/registrations_f.hdf5\"\n",
    "f =  h5py.File(filename, \"r\")\n",
    "a_dataset_keys = list(f.keys()) #extract the different groups of meshes    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = [a_dataset_keys[0],a_dataset_keys[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for data in dataset_list:\n",
    "    data_face = f[data]\n",
    "data_face_arr = np.asarray(data_face)\n",
    "\n",
    "for data in a_dataset_keys[:1]:\n",
    "    data_i = f[data]\n",
    "data_i_arr = np.asarray(data_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00000009'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_index = \"%08d\"%9\n",
    "file_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3d_mesh = o3d.geometry.TriangleMesh()\n",
    "o3d_mesh.vertices = o3d.utility.Vector3dVector(data_i_arr[:,:,0]) # verify what is the name of the attribute for vertices in data dictionary. It should be vertices as given in load method in MeshField class.\n",
    "o3d_mesh.triangles = o3d.utility.Vector3iVector(data_face_arr)\n",
    "o3d.io.write_triangle_mesh(\"registered_dfaust.ply\",o3d_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset of CaDeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset.oflow_dataset as oflow_dataset\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a class for custom dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumansDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_folder,\n",
    "        fields,\n",
    "        split=None,\n",
    "        categories=None,\n",
    "        no_except=True,\n",
    "        transform=None,\n",
    "        length_sequence=17,\n",
    "        n_files_per_sequence=-1,\n",
    "        offset_sequence=0,\n",
    "        ex_folder_name=\"pcl_seq\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Attributes\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.fields = fields\n",
    "        self.no_except = no_except\n",
    "        self.transform = transform\n",
    "        self.length_sequence = length_sequence\n",
    "        self.n_files_per_sequence = n_files_per_sequence\n",
    "        self.offset_sequence = offset_sequence\n",
    "        self.ex_folder_name = ex_folder_name\n",
    "       \n",
    "        # Read metadata file\n",
    "        metadata_file = os.path.join(dataset_folder, \"metadata.yaml\")\n",
    "\n",
    "        \n",
    "        self.metadata = {c: {\"id\": c, \"name\": \"n/a\"} for c in categories}\n",
    "\n",
    "        # Set index\n",
    "        for c_idx, c in enumerate(categories):\n",
    "            self.metadata[c][\"idx\"] = c_idx #only one category: D-FAUST. contains single ID only\n",
    "\n",
    "        # Get all models\n",
    "        self.models = []\n",
    "        for c_idx, c in enumerate(categories):\n",
    "            subpath = os.path.join(dataset_folder, c) #subpath: /usr/stud/srinivaa/code/new_CaDeX/CaDeX/resource/data/Humans/D-FAUST\n",
    "           \n",
    "            if split is not None and os.path.exists(os.path.join(subpath, split + \".lst\")):\n",
    "                split_file = os.path.join(subpath, split + \".lst\") # for train mode: /usr/stud/srinivaa/code/new_CaDeX/CaDeX/resource/data/Humans/D-FAUST/train.lst\n",
    "                with open(split_file, \"r\") as f:\n",
    "                    models_c = f.read().split(\"\\n\") # All files in train.lst for training mode\n",
    "           \n",
    "            models_c = list(filter(lambda x: len(x) > 0, models_c))\n",
    "            models_len = self.get_models_seq_len(subpath, models_c) # gives the total number .npz files in each model\n",
    "            models_c, start_idx = self.subdivide_into_sequences(models_c, models_len)\n",
    "            self.models += [\n",
    "                {\"category\": c, \"model\": m, \"start_idx\": start_idx[i]}\n",
    "                for i, m in enumerate(models_c)\n",
    "            ]\n",
    "        \n",
    "       \n",
    "\n",
    "    def __len__(self):\n",
    "       return len(self.models)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        category = self.models[idx][\"category\"]\n",
    "        model = self.models[idx][\"model\"]\n",
    "        start_idx = self.models[idx][\"start_idx\"]\n",
    "        c_idx = self.metadata[category][\"idx\"]\n",
    "\n",
    "        model_path = os.path.join(self.dataset_folder, category, model)\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        \n",
    "        for field_name, field in self.fields.items():\n",
    "            field_data = field.load(model_path, idx, c_idx, start_idx)\n",
    " \n",
    "            if isinstance(field_data, dict):\n",
    "                for k, v in field_data.items():\n",
    "                    if k is None:\n",
    "                        data[field_name] = v\n",
    "                    else:\n",
    "                        data[\"%s.%s\" % (field_name, k)] = v\n",
    "            else:\n",
    "                data[field_name] = field_data\n",
    "           \n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        \n",
    "        \n",
    "        return data\n",
    "\n",
    "    def get_models_seq_len(self, subpath, models):\n",
    "        \"\"\"Returns the sequence length of a specific model.\n",
    "\n",
    "        This is a little \"hacky\" as we assume the existence of the folder\n",
    "        self.ex_folder_name. However, in our case this is always given.\n",
    "\n",
    "        Args:\n",
    "            subpath (str): subpath of model category\n",
    "            models (list): list of model names\n",
    "        \"\"\"\n",
    "        ex_folder_name = self.ex_folder_name\n",
    "        models_seq_len = []\n",
    "        for m in models:\n",
    "            _sublist = [\n",
    "                f for f in os.listdir(os.path.join(subpath, m, ex_folder_name)) if \"_\" not in f\n",
    "            ]\n",
    "            models_seq_len.append(len(_sublist))\n",
    "        # models_seq_len = [len(os.listdir(os.path.join(subpath, m, ex_folder_name))) for m in models]\n",
    "        return models_seq_len\n",
    "\n",
    "    def subdivide_into_sequences(self, models, models_len):\n",
    "        \"\"\"Subdivides model sequence into smaller sequences.\n",
    "\n",
    "        Args:\n",
    "            models (list): list of model names\n",
    "            models_len (list): list of lengths of model sequences\n",
    "        \"\"\"\n",
    "        length_sequence = self.length_sequence\n",
    "        n_files_per_sequence = self.n_files_per_sequence\n",
    "        offset_sequence = self.offset_sequence\n",
    "\n",
    "        # Remove files before offset\n",
    "        models_len = [l - offset_sequence for l in models_len]\n",
    "\n",
    "        # Reduce to maximum number of files that should be considered\n",
    "        if n_files_per_sequence > 0:\n",
    "            models_len = [min(n_files_per_sequence, l) for l in models_len]\n",
    "\n",
    "        models_out = []\n",
    "        start_idx = []\n",
    "        for idx, model in enumerate(models):\n",
    "            for n in range(0, models_len[idx] - length_sequence + 1):\n",
    "                models_out.append(model)\n",
    "                start_idx.append(n + offset_sequence)\n",
    "\n",
    "        return models_out, start_idx   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    \"\"\"Returns transform objects.\n",
    "\n",
    "    Args:\n",
    "        cfg (yaml config): yaml config object\n",
    "    \"\"\"\n",
    "    n_pcl = 100\n",
    "    n_pt = 512\n",
    "    n_pt_eval = 10000\n",
    "\n",
    "    transf_pt = oflow_dataset.SubsamplePoints(n_pt)\n",
    "    transf_pt_val = oflow_dataset.SubsamplePointsSeq(n_pt_eval, random=False)\n",
    "    transf_pcl_val = oflow_dataset.SubsamplePointcloudSeq(n_pt_eval, random=False)\n",
    "    transf_pcl = oflow_dataset.SubsamplePointcloudSeq(n_pcl, connected_samples=True)\n",
    "\n",
    "    return transf_pt, transf_pt_val, transf_pcl, transf_pcl_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_fields(mode):\n",
    "    \"\"\"Returns data fields.\n",
    "\n",
    "    Args:\n",
    "        mode (str): mode (train|val|test)\n",
    "        cfg (yaml config): yaml config object\n",
    "    \"\"\"\n",
    "    fields = {}\n",
    "    seq_len_train = 17\n",
    "   \n",
    "    seq_len_val = seq_len_train\n",
    "    p_folder = \"points_seq\" # points_seq: contains information regarding the points and their corresponding occupancy values\n",
    "    pcl_folder = \"pcl_seq\" #pcl_seq : contains information regarding the points, scale and loc\n",
    "    mesh_folder = \"mesh_registred\" #mesh_seq: non-existent. Utilize this to have a file containing points and faces for each model.\n",
    "    generate_interpolate = False #False\n",
    "    unpackbits = True # True\n",
    "    \n",
    "    training_all = False\n",
    "    \n",
    "    n_training_frames = 8\n",
    "\n",
    "    # Transformation\n",
    "    transf_pt, transf_pt_val, transf_pcl, transf_pcl_val = get_transforms()\n",
    "\n",
    "    # Fields\n",
    "    pts_iou_field = oflow_dataset.PointsSubseqField\n",
    "    pts_corr_field = oflow_dataset.PointCloudSubseqField\n",
    "\n",
    "    # MeshSubseqField can be used to load mesh fields\n",
    "\n",
    "  \n",
    "    not_choose_last = False\n",
    "    training_multi_files = False\n",
    "    \n",
    "    loss_recon = \"true\"\n",
    "    loss_corr = \"true\"\n",
    "\n",
    "    if mode == \"train\":\n",
    "        if loss_recon:\n",
    "            if training_all:\n",
    "                fields[\"points\"] = pts_iou_field(\n",
    "                    p_folder,\n",
    "                    transform=transf_pt,\n",
    "                    all_steps=True,\n",
    "                    seq_len=seq_len_train,\n",
    "                    unpackbits=unpackbits,\n",
    "                    use_multi_files=training_multi_files,\n",
    "                )\n",
    "            else:\n",
    "                fields[\"points\"] = pts_iou_field(\n",
    "                    p_folder,\n",
    "                    sample_nframes=n_training_frames,\n",
    "                    transform=transf_pt,\n",
    "                    seq_len=seq_len_train,\n",
    "                    fixed_time_step=0,\n",
    "                    unpackbits=unpackbits,\n",
    "                    use_multi_files=training_multi_files,\n",
    "                )\n",
    "            fields[\"points_t\"] = pts_iou_field(\n",
    "                p_folder,\n",
    "                transform=transf_pt,\n",
    "                seq_len=seq_len_train,\n",
    "                unpackbits=unpackbits,\n",
    "                not_choose_last=not_choose_last,\n",
    "                use_multi_files=training_multi_files,\n",
    "            )\n",
    "\n",
    "\n",
    "            fields[\"mesh\"] = oflow_dataset.MeshField(\n",
    "            mesh_folder, seq_len=seq_len_val)\n",
    "\n",
    "    # only training can be boost by multi-files\n",
    "    # modify here, if not train, val should also load the same as the test\n",
    "    else:\n",
    "        fields[\"points\"] = pts_iou_field(\n",
    "            p_folder,\n",
    "            transform=transf_pt_val,\n",
    "            all_steps=True,\n",
    "            seq_len=seq_len_val,\n",
    "            unpackbits=unpackbits,\n",
    "        )\n",
    "        fields[\n",
    "            \"points_mesh\"\n",
    "        ] = pts_corr_field(  # ? this if for correspondence? Checked, this is for chamfer distance, make sure that because here we use tranforms, teh pts in config file must be 100000\n",
    "            pcl_folder, transform=transf_pcl_val, seq_len=seq_len_val\n",
    "        )\n",
    "    # Connectivity Loss:\n",
    "    if loss_corr:\n",
    "        # fields[\"pointcloud\"] = pts_corr_field(\n",
    "        #     pcl_folder,\n",
    "        #     transform=transf_pcl,\n",
    "        #     seq_len=seq_len_train,\n",
    "        #     use_multi_files=training_multi_files,\n",
    "        # )\n",
    "        fields[\"pointcloud\"] = oflow_dataset.MeshField(\n",
    "            mesh_folder, seq_len=seq_len_val)\n",
    "    if mode == \"test\" and generate_interpolate:\n",
    "        fields[\"mesh\"] = oflow_dataset.MeshSubseqField(\n",
    "            mesh_folder, seq_len=seq_len_val, only_end_points=True\n",
    "        )\n",
    "    fields[\"oflow_idx\"] = oflow_dataset.IndexField()\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_field(mode):\n",
    "    \n",
    "    input_type = \"mesh_seq\"\n",
    "    seq_len_train = 17\n",
    "    seq_len_val = seq_len_train\n",
    "    \n",
    "    seq_len = seq_len_train\n",
    "    \n",
    "\n",
    "    if input_type is None:\n",
    "        inputs_field = None\n",
    "    elif input_type == \"img_seq\":\n",
    "        if mode == \"train\" and cfg[\"dataset\"][\"oflow_config\"][\"img_augment\"]:\n",
    "            resize_op = transforms.RandomResizedCrop(\n",
    "                cfg[\"dataset\"][\"oflow_config\"][\"img_size\"], (0.75, 1.0), (1.0, 1.0)\n",
    "            )\n",
    "        else:\n",
    "            resize_op = transforms.Resize((cfg[\"dataset\"][\"oflow_config\"][\"img_size\"]))\n",
    "\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                resize_op,\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if mode == \"train\":\n",
    "            random_view = True\n",
    "        else:\n",
    "            random_view = False\n",
    "\n",
    "        inputs_field = oflow_dataset.ImageSubseqField(\n",
    "            cfg[\"dataset\"][\"oflow_config\"][\"img_seq_folder\"], transform, random_view=random_view\n",
    "        )\n",
    "    elif input_type == \"pcl_seq\":\n",
    "        connected_samples = cfg[\"dataset\"][\"oflow_config\"][\"input_pointcloud_corresponding\"]\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                oflow_dataset.SubsamplePointcloudSeq(\n",
    "                    cfg[\"dataset\"][\"oflow_config\"][\"input_pointcloud_n\"],\n",
    "                    connected_samples=connected_samples,\n",
    "                ),\n",
    "                oflow_dataset.PointcloudNoise(\n",
    "                    cfg[\"dataset\"][\"oflow_config\"][\"input_pointcloud_noise\"]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        training_multi_files = False\n",
    "        if \"training_multi_files\" in cfg[\"dataset\"][\"oflow_config\"]:\n",
    "            if cfg[\"dataset\"][\"oflow_config\"][\"training_multi_files\"] and mode == \"train\":\n",
    "                training_multi_files = True\n",
    "                logging.info(\n",
    "                    \"Oflow D-FAUST PCL Field use multi files to speed up disk performation\"\n",
    "                )\n",
    "\n",
    "        inputs_field = oflow_dataset.PointCloudSubseqField(\n",
    "            cfg[\"dataset\"][\"oflow_config\"][\"pointcloud_seq_folder\"],\n",
    "            transform,\n",
    "            seq_len=seq_len,\n",
    "            use_multi_files=training_multi_files,\n",
    "        )\n",
    "    #TODO : get inputs fields for mesh sequence\n",
    "    elif input_type == \"mesh_seq\":\n",
    "        \n",
    "        # transform = transforms.Compose(\n",
    "        #     [\n",
    "        #         #oflow_dataset.MeshNoise(),\n",
    "        #         #oflow_dataset.DownSampleMesh(N = 512)\n",
    "        #     ]\n",
    "        # )\n",
    "\n",
    "        inputs_field = oflow_dataset.MeshField(\n",
    "            \"mesh_registred\"\n",
    "        )\n",
    "    elif input_type == \"end_pointclouds\":\n",
    "        transform = oflow_dataset.SubsamplePointcloudSeq(\n",
    "            cfg[\"dataset\"][\"oflow_config\"][\"input_pointcloud_n\"],\n",
    "            connected_samples=cfg[\"dataset\"][\"oflow_config\"][\"input_pointcloud_corresponding\"],\n",
    "        )\n",
    "\n",
    "        inputs_field = oflow_dataset.PointCloudSubseqField(\n",
    "            cfg[\"dataset\"][\"oflow_config\"][\"pointcloud_seq_folder\"],\n",
    "            only_end_points=True,\n",
    "            seq_len=seq_len,\n",
    "            transform=transform,\n",
    "        )\n",
    "    elif input_type == \"idx\":\n",
    "        inputs_field = oflow_dataset.IndexField()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input type (%s)\" % input_type)\n",
    "    return inputs_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = get_data_fields(\"train\")\n",
    "inputs_field = get_inputs_field(\"train\")\n",
    "\n",
    "if inputs_field is not None:\n",
    "    fields[\"inputs\"] = inputs_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"/usr/data/cvpr_shared/marvin/Data/CaDeX/data/Humans\"\n",
    "categories = [\"D-FAUST\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a custom dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HumansDataset(\n",
    "        dataset_folder,\n",
    "        fields,\n",
    "        split=\"train\",\n",
    "        categories=categories,\n",
    "        length_sequence=17,\n",
    "        n_files_per_sequence=-1,\n",
    "        offset_sequence=15,\n",
    "        ex_folder_name=\"mesh_registred\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'points': array([[[ 0.06521299, -0.05698125,  0.31752512],\n",
       "         [ 0.25040603, -0.04377124, -0.3058689 ],\n",
       "         [-0.09378615,  0.4495377 , -0.10343115],\n",
       "         ...,\n",
       "         [ 0.16275991, -0.0174749 , -0.16190174],\n",
       "         [-0.45050672,  0.1252174 ,  0.54895157],\n",
       "         [-0.26513195, -0.3668223 , -0.02109268]],\n",
       " \n",
       "        [[ 0.50589174,  0.02797915,  0.20571849],\n",
       "         [-0.35474646,  0.27593046,  0.1856478 ],\n",
       "         [-0.34140444, -0.00873516,  0.27456862],\n",
       "         ...,\n",
       "         [ 0.05789224,  0.53609043, -0.26586908],\n",
       "         [ 0.17120288, -0.28693095, -0.31277567],\n",
       "         [-0.13194604,  0.41042534,  0.52450025]],\n",
       " \n",
       "        [[ 0.5546982 , -0.40892327, -0.04766466],\n",
       "         [-0.1868754 ,  0.00089046,  0.20669077],\n",
       "         [-0.37610364,  0.43072984,  0.47157764],\n",
       "         ...,\n",
       "         [ 0.11464693,  0.55037576, -0.35100394],\n",
       "         [ 0.43035975,  0.3648591 ,  0.21714692],\n",
       "         [ 0.24310677, -0.20663218,  0.45066878]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.07508831,  0.02891167, -0.12244111],\n",
       "         [-0.19810838,  0.21013148,  0.34887552],\n",
       "         [-0.26214927,  0.43607858,  0.54176366],\n",
       "         ...,\n",
       "         [ 0.38484517,  0.04080048, -0.14375311],\n",
       "         [-0.4919513 , -0.18903738,  0.44822386],\n",
       "         [-0.3865686 ,  0.03546114,  0.5018666 ]],\n",
       " \n",
       "        [[-0.02423801,  0.19103146,  0.19491048],\n",
       "         [ 0.30566153, -0.07153599, -0.33096683],\n",
       "         [-0.46244544, -0.35064828,  0.19446048],\n",
       "         ...,\n",
       "         [ 0.17605117,  0.05721331,  0.26603505],\n",
       "         [-0.1952793 ,  0.549987  ,  0.09470126],\n",
       "         [ 0.5368074 , -0.25950775, -0.37943643]],\n",
       " \n",
       "        [[ 0.19528238,  0.29795358, -0.4850348 ],\n",
       "         [-0.5336199 , -0.53309315,  0.31664747],\n",
       "         [-0.35058287,  0.12278467,  0.5676482 ],\n",
       "         ...,\n",
       "         [ 0.18702583,  0.22748956,  0.02637318],\n",
       "         [ 0.16869983, -0.4999138 ,  0.2843704 ],\n",
       "         [-0.10231931,  0.14910561, -0.43968368]]], dtype=float32),\n",
       " 'points.occ': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'points.time': array([0.125 , 0.1875, 0.25  , 0.3125, 0.375 , 0.4375, 0.5   , 0.8125],\n",
       "       dtype=float32),\n",
       " 'points_t': array([[-0.3855987 , -0.47576442, -0.52269113],\n",
       "        [ 0.18895556,  0.2755381 ,  0.47121182],\n",
       "        [ 0.311847  , -0.0424619 ,  0.15847512],\n",
       "        ...,\n",
       "        [ 0.03413253,  0.37860045, -0.30922234],\n",
       "        [-0.52195436,  0.22755657,  0.25566217],\n",
       "        [-0.5209751 , -0.49485913,  0.36521184]], dtype=float32),\n",
       " 'points_t.occ': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " 'points_t.time': array(0.75, dtype=float32),\n",
       " 'mesh.vertices': array([[[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]]], dtype=float32),\n",
       " 'mesh.triangles': array([[[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]]], dtype=int32),\n",
       " 'mesh.time': array([0.    , 0.0625, 0.125 , 0.1875, 0.25  , 0.3125, 0.375 , 0.4375,\n",
       "        0.5   , 0.5625, 0.625 , 0.6875, 0.75  , 0.8125, 0.875 , 0.9375,\n",
       "        1.    ], dtype=float32),\n",
       " 'pointcloud.vertices': array([[[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]]], dtype=float32),\n",
       " 'pointcloud.triangles': array([[[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]]], dtype=int32),\n",
       " 'pointcloud.time': array([0.    , 0.0625, 0.125 , 0.1875, 0.25  , 0.3125, 0.375 , 0.4375,\n",
       "        0.5   , 0.5625, 0.625 , 0.6875, 0.75  , 0.8125, 0.875 , 0.9375,\n",
       "        1.    ], dtype=float32),\n",
       " 'oflow_idx': 1,\n",
       " 'inputs.vertices': array([[[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]],\n",
       " \n",
       "        [[ 0.05081545,  1.113255  , -0.02681933],\n",
       "         [ 0.04889621,  1.1002072 , -0.01560232],\n",
       "         [ 0.06162366,  1.0973535 , -0.02600231],\n",
       "         ...,\n",
       "         [-0.07664309,  1.0625885 , -0.1372281 ],\n",
       "         [-0.0781327 ,  1.0637605 , -0.13475484],\n",
       "         [-0.08680973,  1.0565943 , -0.12819023]]], dtype=float32),\n",
       " 'inputs.triangles': array([[[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]],\n",
       " \n",
       "        [[   1,    2,    0],\n",
       "         [   0,    2,    3],\n",
       "         [   2,    1,    4],\n",
       "         ...,\n",
       "         [4805, 3511, 6309],\n",
       "         [3511, 1330, 6309],\n",
       "         [6309, 1330, 4687]]], dtype=int32),\n",
       " 'inputs.time': array([0.    , 0.0625, 0.125 , 0.1875, 0.25  , 0.3125, 0.375 , 0.4375,\n",
       "        0.5   , 0.5625, 0.625 , 0.6875, 0.75  , 0.8125, 0.875 , 0.9375,\n",
       "        1.    ], dtype=float32)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "start_idx = 0\n",
    "\n",
    "folder = os.path.join(\"/usr/data/cvpr_shared/marvin/Data/CaDeX/data/Humans/D-FAUST/50021_hips\",\"mesh_registred\")\n",
    "mesh_files = glob.glob(os.path.join(folder, \"*.%s\" % \"npz\"))\n",
    "mesh_files.sort()\n",
    "mesh_files = mesh_files[start_idx : start_idx + 17]\n",
    "\n",
    "mesh_vertices_seq = []\n",
    "mesh_face_seq = []\n",
    "\n",
    "for f in mesh_files:\n",
    "    data = np.load(f)\n",
    "    vertices = data['points']\n",
    "    triangles = data['triangles']\n",
    "\n",
    "    mesh_vertices_seq.append(vertices)\n",
    "    mesh_face_seq.append(triangles)\n",
    "\n",
    "data = {\"vertices\":np.stack(mesh_vertices_seq)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_dict = {}\n",
    "dataloader_dict[\"train\"] = DataLoader(\n",
    "                dataset, # keys : points, points_t, mesh, pointcloud, oflow_idx, inputs\n",
    "                batch_size=12,\n",
    "                shuffle=True,\n",
    "                num_workers=2,\n",
    "                pin_memory=False, #set to true to ensure faster data transfer between CPU and GPU. Set it to false only if the data \n",
    "                #directly transferred into GPU and it is really small\n",
    "                drop_last=True,  # ! check this\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study shape class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.utils_arap.shape_utils import Shape\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_1 = Shape(vert=torch.from_numpy(dataset[1]['mesh.vertices']),triv=torch.from_numpy(dataset[1]['mesh.triangles']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_1.get_neigh().type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study ARAP loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cadex_1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00ecdfb374cb07288fc88678eed1090d022d8c5ada305335ac9b4b12e32ebcc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
